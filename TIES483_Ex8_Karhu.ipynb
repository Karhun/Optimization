{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIES483 Exercice 8\n",
    "Anette Karhu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 8:\n",
    "Find an application of optimization in a scienfic paper, and replicate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found one nonlinear journal with multivariable problem. The name of the study is : Comparative study of Optimization methods for\n",
    "Unconstrained Multivariable Nonlinear Programming\n",
    "Problems. \n",
    "\n",
    "Most of the studies found in the internet were outside of the scope of this course and handled evolutionary algortihms. That is why I chose this study as it was the closest topic relevant to the course's scope.\n",
    "\n",
    "The journal can be found from the following address:\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.9756&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results found in the journal are stated below:\n",
    "\n",
    "\"Surprisingly not much attention has been given to maximization of multivariable nonlinear programming problems by the\n",
    "scholars. So we have chosen to study maximization problems and to our pleasant surprise, the results obtained are compatible\n",
    "with theoretical observations. In gradient search methods, the rate of convergence is slow and the result obtained is approximate\n",
    "to the optimal value. But in Newton’s method and Quasi-Newton methods, the rate of convergence is faster and the results are accurate.\" (Neha et al. 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of Gradient search method, Newton’s method and Quasi- Newtons method (BFGS) has been made in the study by using the following multilinear maximization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max f(x)=2x_1x_2 + 2x_2 – x_1^2 – 2x_2^2\n",
    "\\end{align}  \n",
    "$$\n",
    "\n",
    "using Gradient search starting at point $x_0 = (0, 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we implement these methods by using python and it's libraries because the paper did not reveal how they had implemented the results. The purpose is to see whether with python we can get similar results as they have conducted in this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximization function\n",
    "def f(x):\n",
    "    return 2*x[0]*x[1] + 2*x[1] - x[0]**2 -2*x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "# The gradient and hessian for the problem\n",
    "grad, hess = ad.gh(f)\n",
    "x = [0,0]\n",
    "print(grad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient search aka gradient ascent method\n",
    "def gradient_search(f, x, step, iters):\n",
    "    f_old = float('Inf')\n",
    "    f_next = f(x)\n",
    "    d = float('Inf')\n",
    "    for i in range(0,iters):\n",
    "        f_old = f_next\n",
    "        d = np.array(grad(x))\n",
    "        x = x+d*step\n",
    "        f_next = f(x)\n",
    "    return x,f_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution with gradient descent:  [0.97797853 0.98638998] function value: 0.9997440148897343\n",
      "Iterations amount:  50\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0,0])\n",
    "iters = 50\n",
    "(x_opt,f_opt) = gradient_search(f,x,0.1, iters)\n",
    "print(\"Optimal solution with gradient descent: \",x_opt, 'function value:', f_opt)\n",
    "print('Iterations amount: ', iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the function given in the paper with python version of gradient descent, we get similar results as in the paper. In the paper the conclusion is that the results converge to point were $x = (1,1)$. We can see in this as well that with less iterations (50) we are almost at that point but not yet. If we increase the iterations (1000) we get to the optimal point $x=(1,1)$. Also we can see the same result as in the paper, that gradient search method requires lot of iterations to gain exact results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton(f,x,step):\n",
    "    f_start = float('Inf')\n",
    "    iters = []\n",
    "    f_next = f(x)\n",
    "    while abs(f_start-f_next)>0.001:\n",
    "        f_start = f_next\n",
    "        inverse = np.linalg.inv(np.matrix(ad.gh(f)[1](x)))\n",
    "        d = (-inverse*(np.matrix(ad.gh(f)[0](x)).transpose())).transpose()\n",
    "        x = np.array(x+d*step)[0]\n",
    "        f_next = f(x)\n",
    "        iters.append(list(x))\n",
    "    return x,f_next,iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution with Newtons method:  [1. 1.] with function value: 1.0\n",
      "Number of iterations: 2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.0,0.0])\n",
    "(x_opt,f_opt,iters) = newton(f,x,1)\n",
    "print(\"Optimal solution with Newtons method: \",x_opt, 'with function value:', f_opt)\n",
    "print('Number of iterations:', len(iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton method gave the same result as in the research paper, the optimal result for the problem is $x=(1,1)$. Only difference was, that the method in the paper got the optimal solution with one step, as with python this took 2 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, -2.0]\n"
     ]
    }
   ],
   "source": [
    "# Because we use scipys minimize function for maximization\n",
    "# we need to convert the problem to minimizable funtion, -(f(x))\n",
    "x=[0,0]\n",
    "def f_neg(x):\n",
    "    return -(2*x[0]*x[1] + 2*x[1] - x[0]**2 -2*x[1]**2)\n",
    "\n",
    "# Take gradient and hessian of the -(f(x))\n",
    "ngrad, nhess = ad.gh(f_neg)\n",
    "print(ngrad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quasi- Newton method\n",
    "def quasi_newton(f, x, ngrad):\n",
    "    res = minimize(f_neg,(0,0), method=\"BFGS\", jac=ngrad,\n",
    "               options={\"gtol\": 1e-6, 'disp': True})\n",
    "    return res, res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.000000\n",
      "         Iterations: 2\n",
      "         Function evaluations: 4\n",
      "         Gradient evaluations: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      fun: -1.0\n",
       " hess_inv: array([[1. , 0.5],\n",
       "       [0.5, 0.5]])\n",
       "      jac: array([0., 0.])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 4\n",
       "      nit: 2\n",
       "     njev: 4\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([1., 1.]),\n",
       " array([1., 1.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quasi_newton(f_neg,x,ngrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipys minimize function we use here can be used as maximize, because as we know min $-f(x)$ is the same as max $f(x)$. As we see by converting the problem to negative, $-f(x)$, we get the optimal solution that was also found in the paper where $x=(1,1)$. The iterations with python took one step more, as they managed in the paper to iterate to the optimal solution with just one step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The papers application was succesfully tested and I managed to implement the journals methods similarly. I gained similar and even the same results as the paper have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
