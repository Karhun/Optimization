{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIES483 Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anette Karhu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 2.1 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that that you are buying and selling electricity. The more electricity you buy, the better price you get per kwH and the buying prize of electricity follows function $f(x) = 1-0.01x$, where $0\\leq x\\leq50$ is the amount of electricity you buy. On the other hand, the price that you get from selling electricity follows function $g(x) = 2-0.01x^2$ with $x$ again the amount of electricity that you sell. Formulate an optimization problem that maximizes the profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate profit with functions f and g. \n",
    "\n",
    "$$\n",
    "P(x) = g(x)x-f(x)x\\\\\n",
    "P(x) = (2-0.01x^2)x-(1-0.01x)x \\\\\n",
    "P(x) = -0.01x^3+0.01x^2+x\n",
    "$$\n",
    "\n",
    "In optimization problem format this looks like:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max -0.01x^3+0.01x^2+x\\\\\n",
    "\\text{s.t. }x\\in [0,50].\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we know: \n",
    "$$\n",
    "\\max h(x) = \\min -(h(x))\n",
    "$$\n",
    "\n",
    "and applying this to our maximization function, we get minimization function, which would look like:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min -(-0.01x^3+0.01x^2+x)\\\\\n",
    "\\text{s.t. }x\\in [0,50].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and thus our maximization of our profit is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min 0.01x^3-0.01x^2-x\\\\\n",
    "\\text{s.t. }x\\in [0,50].\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ex. 2.2 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use bisection search to optimize problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min  (1-x)^2+x\\\\\n",
    "\\text{s.t. }x\\in [0,2].\n",
    "\\end{align}\n",
    "$$ Code the search algorithm by yourself, not use the one shown in the lecture material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function used in ex 2.2 and 2.3\n",
    "# =============================================================================\n",
    "def f(x):\n",
    "    return (1-x)**2 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bisection optimization result: 0.750005 function value is 0.812502500025\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Ex 2.2 Bisection search \n",
    "# =============================================================================\n",
    "def bisection(a,b,f,epsilon):\n",
    "    c = f(a+b)/2\n",
    "    for i in range(0,1000000):\n",
    "        if f(c) - epsilon < f(c) + epsilon:\n",
    "            b = c + epsilon\n",
    "        elif f(c) - epsilon > f(c) + epsilon:\n",
    "            a = c - epsilon\n",
    "    return (a+b)/2\n",
    "\n",
    "\n",
    "x = bisection(0,2,f,0.00001)\n",
    "print('Bisection optimization result:',x, 'function value is', f(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ex. 2.3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use golden section search to optimize the problem from above exercise. Code the search algorithm by yourself, not use the one shown in the lecture material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden ratio search result: 0.5000000035355098 function value is 0.75\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# =============================================================================\n",
    "# Ex. 2.3 Golden ratio search\n",
    "# =============================================================================\n",
    "def golden_search(a,b,f):\n",
    "    goldenratio = (math.sqrt(5.0)-1.0)/2.0\n",
    "    for i in range(0,100000):\n",
    "        d = goldenratio *(b-a)\n",
    "        x1 = a+d\n",
    "        x2 = b-d\n",
    "        if f(x2) > f(x1):\n",
    "            a = x2      \n",
    "        elif f(x2) < f(x1):\n",
    "            b = x1      \n",
    "    return (a+b)/2.0\n",
    "\n",
    "x1 = golden_search(0,2,f)\n",
    "print('Golden ratio search result:',x1, 'function value is', f(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 2.4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use differentiation to optimize the optimization problem of exercise 1 and to verify the answer of optimization problem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's find the maximum of the function: $$P(x)= -0.01x^3+0.01x^2+x$$\n",
    "We can do this by derivating by hand. The derivate of the funtion is:\n",
    "\n",
    "$$\n",
    "P'(x)=-0.03x^2+0.02x+1\\\\\n",
    "-3/100x^2+1/50x+1=0,\\\\\n",
    "$$\n",
    "which is true when x = -5.449.. or x = 6.116..\n",
    "And the function value where the slope is zero is approximately:\n",
    "\n",
    "$$ \n",
    "P(-5,449..) = -3.533.. \\\\\n",
    "P(6.116..) = 4.202..\\\\\n",
    "$$\n",
    "\n",
    "And thus calculating the second derivate, we can approximate these critical points where the first derivate is zero.\n",
    "\n",
    "$$ P''(x)=-6/100x+1/50 \\\\\n",
    "P''(6.1..) = -0.34.. < 0 => local max\\\\\n",
    "P''(-5.4..) = 0.34.. > 0 => local min\\\\\n",
    "$$\n",
    "Because we are looking for a local max and because x is defined only in the interval [0,50] and we can ignore the local min which is the point x = -5.4..\n",
    "\n",
    "Thus the second derivate have shown us that the local max for the problem is x =6.1164..\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And the same for the second problem:\n",
    "    \n",
    "$$\n",
    "P(x) = (1-x)^2+x \\\\\n",
    "P(x) = x^2-x+1 \\\\\n",
    "P'(x)=2x-1\\\\\n",
    "2x-1= 0 => x = 1/2\\\\\n",
    "P(1/2) = 3/4 = 0.75\\\\\n",
    "P''(x)=2\\\\\n",
    "P''(1/2)=2 > 0\\\\\n",
    "$$\n",
    "\n",
    "Thus local minima has been found with second derivate rule, and the local minimun point is x = 1/2 = 0.5 and function value is P(1/2)=0.75 which is the result from ex 2.3. Ex 2.2 on the other hand is a bit less accurate for this problem even with large iteration numbers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
